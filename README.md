# Q学习算法
强化学习算法实现了一个确定性的《冰湖问题》（FrozenLake），这是一种“网格世界”问题，其中Q学习智能体通过学习一个预定的策略，能够在冰湖中找到最佳路径。程序使用Python编写了两个类：一个用于设置环境状态，另一个用于设置智能体。Q值表示状态-动作对的价值，算法通过根据这些Q值来选择当前状态下的最佳动作。执行该动作后，智能体会观察到相应的奖励和下一状态，并根据这些信息更新Q值。通过多次迭代，算法能够学习出最优路径，只要能够正确平衡“探索”（Exploration）和“利用”（Exploitation）。

**网格示意图：**

![网格](https://github.com/ronanmmurphy/Q-Learning-Algorithm/blob/main/Images/grid.PNG?raw=true)

**方法：**
Q学习算法采用了“ε贪心”方法（epsilon greedy），即10%的时间随机选择动作，其余90%的时间选择最佳动作。Q值的计算公式如下：
\[
Q\_值 = (1-\alpha) \times Q[(i,j,动作)] + \alpha \times (奖励 + \gamma \times Q_{max}[下一个状态动作])
\]
其中：
- \(\alpha\) 是学习率（即Q值更新的步长）
- \(\gamma\) 是折扣因子，用于权衡未来奖励与当前奖励
- \(Q_{max}\) 是下一状态的最大Q值

每个回合中的Q值都会根据公式进行计算和更新。如果当前状态是结束状态，Q值将设为对应的奖励值——失败时为-5，成功时为+1，并且状态会重置为初始状态(0,0)。

**最优解：**

![最优解](https://github.com/ronanmmurphy/Q-Learning-Algorithm/blob/main/Images/optimal_solution.PNG?raw=true)

**每回合奖励变化：**
为了观察奖励的变化，算法在10,000个回合中运行。结果显示，尽管算法刚开始时表现不佳，但它逐渐快速地学会了最佳路径。由于算法在10%的时间内会执行随机动作，因此它并不会总是停留在最优解上。为了优化这个过程，可以考虑随着训练的进行减少探索的比例，从而让算法更专注于利用当前已学到的最优策略。

![每回合奖励](https://github.com/ronanmmurphy/Q-Learning-Algorithm/blob/main/Images/RewardPerEpisode.png?raw=true)

在这段 Q-learning 算法代码中，计算出的矩阵 `Q` 是一个 Q-table，存储了每个状态-动作对的 Q 值，Q 值代表了在某一状态下采取某一动作的预期长期回报。具体来说，它包含了从任意状态 `(i, j)` 开始，执行动作 `a` 后的 Q 值。

### Q-table 的含义：
- **状态 (State)**：在这个问题中，状态是一个 `(i, j)` 坐标，表示代理在 5x5 网格中的位置。
- **动作 (Action)**：每个状态下代理可以选择四个动作之一：[上, 下, 左, 右]，在代码中分别由 0, 1, 2, 3 表示。
- **Q 值**：Q 值存储了状态-动作对的值，计算公式为：

  \[
  Q(s, a) = (1 - \alpha) \times Q(s, a) + \alpha \times \left( r + \gamma \times \max_{a'} Q(s', a') \right)
  \]

  其中：
  - \( \alpha \) 是学习率，控制着新信息对旧信息的影响。
  - \( \gamma \) 是折扣因子，表示未来奖励的折扣程度。
  - \( r \) 是当前状态下采取动作后的即时奖励。
  - \( \max_{a'} Q(s', a') \) 是从新状态 \( s' \) 经过动作 \( a' \) 后的最大 Q 值，表示从新状态出发，代理能够获得的最大预期奖励。

### 如何使用 Q-table：
1. **策略 (Policy)**：Q-table 是代理学习如何在网格中采取最佳行动的依据。代理根据 Q 值来选择动作。通常，如果 Q 值越大，表示该动作在该状态下更好（长期回报更高）。在选择动作时，代理可以采用 **epsilon-greedy 策略**，即有一定概率选择随机动作（探索），有较高概率选择 Q 值最大的动作（利用）。
   
2. **探索与利用**：通过 Q-learning 算法，代理在训练过程中逐渐更新 Q-table，使其对每个状态-动作对的 Q 值越来越准确，最终得到一组最优的策略。

3. **决策支持**：经过足够的训练，Q-table 中的值能够指引代理在任意状态下做出最优决策。例如，从起始状态 `(0, 0)` 出发，代理将选择能够带来最大长期回报的动作，从而在最短的时间内到达胜利状态 `(4, 4)`。

### Q-table 的实际应用：
- **找到最优路径**：通过 Q-learning 训练，最终的 Q-table 会反映出从任何位置到达胜利状态 `(4, 4)` 的最优路径。你可以查看每个状态下最大 Q 值对应的动作，从而找到一个最优的行动策略。
- **实时决策**：在实际应用中，Q-table 可以被用来帮助代理在动态环境中实时做出最优决策。如果环境变化，代理可以通过重新训练 Q-table 来适应新环境。

### 代码中的 Q-table 展示：
在 `display_q_values()` 方法中，Q-table 被输出显示，其中每个格子中显示的是状态 `(i, j)` 下各个动作对应的最大 Q 值。通过观察这个矩阵，可以分析出哪些位置是“危险”的（例如掉入陷阱的状态），哪些位置是“有利”的（例如靠近胜利状态的状态）。


当你在网格的 `(1, 3)` 位置时，Q-table 中该格子的数字代表的是在 `(1, 3)` 状态下，采取不同动作（上、下、左、右）后的最大长期回报（Q 值）。具体来说，Q-table 存储了每个状态-动作对的 Q 值，它反映了在特定状态下采取某一动作所获得的预期回报。

### 具体来说：
假设你在 `(1, 3)` 位置，Q-table 记录了从 `(1, 3)` 状态出发，分别采取四个动作后（上、下、左、右）的预期回报。每个动作都会导致一个新的状态（即一个新的 `(i, j)` 坐标），然后根据状态转移的结果和奖励机制，更新 Q 值。

假设在 `(1, 3)` 位置的 Q-table 显示如下：

```
| 0.000 | 0.000 | 0.000 | 0.000 | 0.000 |
| 0.000 | 0.000 | 0.000 | 0.200 | 0.000 |
| 0.000 | 0.000 | 0.000 | 0.000 | 0.000 |
| 0.000 | -5.000| 0.000 | 0.000 | 0.000 |
| 0.000 | 0.000 | 0.000 | 0.000 | 1.000 |
```

### 在 `(1, 3)` 时，格子的 Q 值代表：
- **最大 Q 值：** 假设在 `(1, 3)` 状态下，四个动作（上、下、左、右）的 Q 值分别为：
  - 上：Q 值为 `-1`（假设的值，根据奖励机制和状态转移）。
  - 下：Q 值为 `-1`。
  - 左：Q 值为 `0.0`（假设的值）。
  - 右：Q 值为 `0.2`。

  那么在 `(1, 3)` 状态下，Q-table 中的数字是 **`0.200`**，表示从 `(1, 3)` 位置出发，右移（动作 3）是最优的动作，因为右移后的 Q 值最高。

### 具体的行动解释：
- **状态 `(1, 3)` 的奖励：** 在 `(1, 3)` 状态，假设执行动作后的奖励为：
  - 如果选择 **右**（动作 3），则代理将移动到 `(1, 4)` 位置，假设此位置是一个普通位置，Q 值在之前的学习过程中更新为 `0.2`，这反映了右移后的较好回报。
  - 如果选择 **上**（动作 0），代理将移到 `(0, 3)`，假设该位置没有额外奖励，Q 值可能为 `-1`。
  - 如果选择 **下**（动作 1），代理将移到 `(2, 3)`，假设该位置也没有更好的回报，Q 值可能为 `-1`。
  - 如果选择 **左**（动作 2），代理将移到 `(1, 2)`，如果这个位置的奖励也较低，Q 值可能为 `0`。

### 训练过程中的动态更新：
每次 Q-learning 更新时，代理会根据当前状态的奖励、以及到达下一状态后的最大 Q 值来更新当前 Q 值。例如，当代理从 `(1, 3)` 移动到 `(1, 4)` 后，假设 `(1, 4)` 的最大 Q 值为 `0.5`，并且奖励为 `-1`，Q 值会根据公式进行更新：

\[
Q((1, 3), \text{right}) = (1 - \alpha) \times Q((1, 3), \text{right}) + \alpha \times \left( r + \gamma \times \max_a Q((1, 4), a) \right)
\]

如果学习率 `α` 为 `0.5`，折扣因子 `γ` 为 `0.9`，那么更新后的 Q 值可能变为：

\[
Q((1, 3), \text{right}) = (1 - 0.5) \times 0.0 + 0.5 \times \left( -1 + 0.9 \times 0.5 \right) = 0.5 \times (-1 + 0.45) = 0.5 \times -0.55 = -0.275
\]

这会逐步影响 Q-table 中的值，并最终帮助代理学习到最优的策略。

### 总结：
- 在 `(1, 3)` 状态，Q-table 中的数字代表从该位置出发，采取不同动作的长期预期回报（Q 值）。
- Q 值越高，意味着该动作的长期回报越好，因此代理在选择动作时会倾向于选择 Q 值最大的动作。
- 通过多次训练，代理将不断更新 Q-table，最终形成一张最优的决策表，帮助其在网格中找到最短路径或最佳策略。
